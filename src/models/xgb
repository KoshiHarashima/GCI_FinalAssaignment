"""XGBoost regression wrapper with simple timeâ€‘series split search."""
from __future__ import annotations

import xgboost as xgb
import numpy as np
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error


BASE_PARAMS = {
    "objective": "reg:squarederror",
    "eval_metric": "rmse",
    "booster": "gbtree",
    "seed": 42,
}


SEARCH_SPACE = {
    "max_depth": [4, 6, 8],
    "eta": [0.01, 0.05, 0.1],
    "subsample": [0.6, 0.8, 1.0],
    "colsample_bytree": [0.6, 0.8, 1.0],
}


def _grid(params, space):
    import itertools

    keys = space.keys()
    for combo in itertools.product(*space.values()):
        yield {**params, **dict(zip(keys, combo))}


def tune_and_train(X_train, y_train, num_round: int = 800):
    dtrain = xgb.DMatrix(X_train, y_train)
    best_rmse, best_params = 1e9, None
    for p in _grid(BASE_PARAMS, SEARCH_SPACE):
        cv = xgb.cv(p, dtrain, num_boost_round=400, nfold=5, early_stopping_rounds=30, verbose_eval=False)
        rmse = cv["test-rmse-mean"].min()
        if rmse < best_rmse:
            best_rmse, best_params = rmse, p
    model = xgb.train(best_params, dtrain, num_boost_round=num_round)
    return model, best_rmse


def evaluate(model, X_test, y_test):
    preds = model.predict(xgb.DMatrix(X_test))
    rmse = mean_squared_error(y_test, preds, squared=False)
    mae = np.mean(np.abs(y_test - preds))
    return {"MAE": float(mae), "RMSE": float(rmse)}
